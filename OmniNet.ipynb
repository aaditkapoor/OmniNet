{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <u>If only Ben's Omnitrix had Machine Learning inbuilt?</u>\n",
    "<strong> Imagine if the Omnitrix was capable of automatically turning Ben into the alien with the highest possibility of defeating the enemy</strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 871,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import zscore\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 872,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the data\n",
    "df = pd.read_csv(\"Ben_10_Data_Set.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 873,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Character</th>\n",
       "      <th>Power_Level</th>\n",
       "      <th>Ben_10_Series</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>YOUNG MAX</td>\n",
       "      <td>39.5</td>\n",
       "      <td>Ben 10: Classic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>VILGAX</td>\n",
       "      <td>80</td>\n",
       "      <td>Ben 10: Classic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BEN TENNYSON</td>\n",
       "      <td>4</td>\n",
       "      <td>Ben 10: Classic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>GWEN TENNYSON</td>\n",
       "      <td>7</td>\n",
       "      <td>Ben 10: Classic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>GRANDPA MAX</td>\n",
       "      <td>17.2</td>\n",
       "      <td>Ben 10: Classic</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Character Power_Level    Ben_10_Series\n",
       "0      YOUNG MAX        39.5  Ben 10: Classic\n",
       "1         VILGAX          80  Ben 10: Classic\n",
       "2   BEN TENNYSON           4  Ben 10: Classic\n",
       "3  GWEN TENNYSON           7  Ben 10: Classic\n",
       "4    GRANDPA MAX        17.2  Ben 10: Classic"
      ]
     },
     "execution_count": 873,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let us turn all the categorical into labels\n",
    "<strong> We will initialize a LabelEncoder to achieve this. </strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 874,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length [Ben_10_Series]: 4\n",
      "Length [Character]: 90\n"
     ]
    }
   ],
   "source": [
    "lr = LabelEncoder()\n",
    "\n",
    "def convert_id_to_category(index, mapping=None):\n",
    "    \"\"\"\n",
    "        Convert the id into the label\n",
    "        Arguments:\n",
    "            mappping: The Mapping built by the LabelEncoder\n",
    "            id: id corresponding to the label\n",
    "        Returns:\n",
    "            str: the label\n",
    "    \"\"\"\n",
    "    return mapping.get(index, False)\n",
    "\n",
    "def convert_to_mapping(lr:LabelEncoder):\n",
    "    \"\"\"\n",
    "        Extract the mapping from the LabelEncoder\n",
    "        \n",
    "        Returns:\n",
    "            Dict: key/value for the label encoded\n",
    "    \"\"\"\n",
    "    mapping = dict(list(zip(lr.transform(lr.classes_),lr.classes_)))\n",
    "    return mapping\n",
    "\n",
    "\n",
    "def get_power_level_mapping(df=None):\n",
    "    mapping = {}\n",
    "    for i in range(0, len(df)):\n",
    "        mapping[df.loc[i].Character] = df.loc[i].Power_Level\n",
    "    vtok = {}\n",
    "    for i,j in enumerate(mapping):\n",
    "        vtok[mapping[j]] = j\n",
    "    return mapping, vtok\n",
    "\n",
    "# Ben_10_Series\n",
    "df['Ben_10_Series'] = lr.fit_transform(df['Ben_10_Series'])\n",
    "mapping_ben_10_series = convert_to_mapping(lr)\n",
    "df['Character'] = lr.fit_transform(df['Character'])\n",
    "mapping_character = convert_to_mapping(lr)\n",
    "\n",
    "print (\"Length [Ben_10_Series]: {}\".format(len(mapping_ben_10_series)))\n",
    "print (\"Length [Character]: {}\".format(len(mapping_character)))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 877,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                            39.5\n",
       "1                              80\n",
       "2                               4\n",
       "3                               7\n",
       "4                            17.2\n",
       "                 ...             \n",
       "92                            520\n",
       "93    NORMAL = 12.5, FACE = 1,340\n",
       "94                            336\n",
       "95                          2,177\n",
       "96      1,080,000,000,000,000,000\n",
       "Name: Power_Level, Length: 97, dtype: object"
      ]
     },
     "execution_count": 877,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Power_Level']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 875,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Character</th>\n",
       "      <th>Power_Level</th>\n",
       "      <th>Ben_10_Series</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>89</td>\n",
       "      <td>39.5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>77</td>\n",
       "      <td>80</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>39</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>32</td>\n",
       "      <td>17.2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Character Power_Level  Ben_10_Series\n",
       "0         89        39.5              1\n",
       "1         77          80              1\n",
       "2         10           4              1\n",
       "3         39           7              1\n",
       "4         32        17.2              1"
      ]
     },
     "execution_count": 875,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <strong> Categorized data </strong>\n",
    "- All category labels have been turned to numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <u>Data Cleaning</u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 878,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_string_powerlevel(df=None):\n",
    "    \"\"\"\n",
    "        Replaces the string format of power level into an integer. (Manually checked the data)\n",
    "        \n",
    "        Arguments:\n",
    "            df: Pandas DataFrame\n",
    "        Returns\n",
    "            None\n",
    "    \"\"\"\n",
    "    \n",
    "    # lowe bound\n",
    "    df.loc[28, \"Power_Level\"] = \"265\"\n",
    "    df.loc[93, \"Power_Level\"] = \"12.5\"\n",
    "    df.loc[51, \"Power_Level\"] = \"195\"\n",
    "    df.loc[52, \"Power_Level\"] = \"160\"\n",
    "    df.loc[62, \"Power_Level\"] = \"140\"\n",
    "    df.loc[67, \"Power_Level\"] = \"20\"\n",
    "    df['Power_Level'] = df['Power_Level'].str.replace(\",\",\"\")\n",
    "    \n",
    "    # converting power_level to float\n",
    "    df['Power_Level'] = df['Power_Level'].astype(float)\n",
    "    df['Character'] = df['Character'].astype(int)\n",
    "    \n",
    "\n",
    "remove_string_powerlevel(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Power_Level contained some uncleaned values (string)\n",
    "- Power_Level contained values in string\n",
    "- Power_Level contained commas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 882,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 882,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.scatterplot(data=df['Power_Level'].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Now our data is cleaned, we will start by doing some feature transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Transformation\n",
    "- In addition to feature transformations, we will create a new dataset capable of handling multiclass classification problems.\n",
    "- To see where we can apply feature transformations, we will first check for any outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 771,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "def split_powerlevel(df, factor=3):\n",
    "    \"\"\"\n",
    "        Split the power level by the specified factor\n",
    "        Arguments:\n",
    "            df:DataFrame\n",
    "            factor: int\n",
    "        Returns:\n",
    "            data: dict[str] = list[str]\n",
    "            \n",
    "    \"\"\"\n",
    "    data = {'air_power1':[], 'sand_power1':[], 'water_power1':[], \"air_power2\":[], \"sand_power2\":[], \"water_power2\":[]}\n",
    "    for i in df['power_level1']:\n",
    "        t = i/factor # float\n",
    "        data['air_power1'].append(t)\n",
    "        data['sand_power1'].append(t)\n",
    "        data['water_power1'].append(t)\n",
    "    for i in df['power_level2']:\n",
    "        t = i/factor # float\n",
    "        data['air_power2'].append(t)\n",
    "        data['sand_power2'].append(t)\n",
    "        data['water_power2'].append(t)\n",
    "    return data\n",
    "\n",
    "\n",
    "def create_data(df, split_size=0.2, size=1000):\n",
    "    \"\"\"\n",
    "        Create Data for Multiclass classification problem\n",
    "        Arguments:\n",
    "            df: Pandas DataFrame\n",
    "        Returns:\n",
    "            data: Pandas DataFrame\n",
    "    \"\"\"\n",
    "    def return_individual_data(th = .5, winner='c1'):\n",
    "        \"\"\"\n",
    "            Returns a row in our dataset.\n",
    "            \n",
    "            Arguments:\n",
    "                th: threshold\n",
    "                winner: The winning character\n",
    "                \n",
    "            Returns:\n",
    "                (str,str, float, float, str)\n",
    "        \"\"\"\n",
    "        characters = np.array(list(mapping_character.keys()))\n",
    "        mapping_ch_to_pl, mapping_pl_to_ch = get_power_level_mapping(df)\n",
    "        random_character1 = np.random.choice(characters)\n",
    "        random_character2 = np.random.choice(characters)\n",
    "        p1 = mapping_ch_to_pl[random_character1]\n",
    "        p2 = mapping_ch_to_pl[random_character2]\n",
    "        power_diff = np.abs(mapping_ch_to_pl[random_character1] - mapping_ch_to_pl[random_character2])\n",
    "        if winner == 'c1':\n",
    "            return random_character1, random_character2,p1, p2, random_character1\n",
    "        else:\n",
    "            return random_character1, random_character2,p1, p2, random_character2\n",
    "\n",
    "            \n",
    "\n",
    "    # win is by character1 (by default)\n",
    "    data = {'character1':[], 'character2':[], 'power_level1':[],'power_level1':[],'power_level2':[], 'win':[]}\n",
    "    \n",
    "    # first half\n",
    "    for i in range(0, size//2):\n",
    "        c1, c2, p1, p2, c1 = return_individual_data(winner='c1')\n",
    "        data['character1'].append(c1)\n",
    "        data['character2'].append(c2)\n",
    "        data['power_level1'].append(p1)\n",
    "        data['power_level2'].append(p2)\n",
    "        data['win'].append(c1)\n",
    "    \n",
    "    # second half\n",
    "    for i in range(0, size//2):\n",
    "        c1, c2, p1, p2, c2 = return_individual_data(winner='c2')\n",
    "        data['character1'].append(c1)\n",
    "        data['character2'].append(c2)\n",
    "        data['power_level1'].append(p1)\n",
    "        data['power_level2'].append(p2)\n",
    "        data['win'].append(c2)\n",
    "    \n",
    "    data_df = pd.DataFrame(data=data, columns=['character1', 'character2', 'power_level1', 'power_level2', 'win'])\n",
    "    data_df = shuffle(data_df)\n",
    "    \n",
    "    toadd = split_powerlevel(data_df)\n",
    "    for k,v in enumerate(toadd):\n",
    "        data_df[v] = toadd[v]\n",
    "    \n",
    "    features, labels = data_df.drop(columns=['win']).values, data_df.win.values\n",
    "    \n",
    "    x_train, x_test, y_train, y_test = train_test_split(features, labels, test_size=0.2, random_state=42)\n",
    "    x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.25, random_state=42)\n",
    "    \n",
    "    \n",
    "    print (\"Generated data of size:{}\".format(size))\n",
    "\n",
    "    return features, labels,x_train, x_test, y_train, y_test,x_val, y_val, data_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Power_Level'] = np.log(df['Power_Level'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We can clearly see that there are two outliers and the obvious way to check for outlieris to analyze thresholds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot(data=df['Power_Level'].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Log Transformation of the data of outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Power_Level'].var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values = df['Power_Level'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- If we set the threshold as the mean of the zscore values, we can clearly see all the outliers that are True.\n",
    "- Now we do not want to remove the values as each character is essential.\n",
    "- If we view the scatterplot of our data, we can clearly see the outlier very clearly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zscore(values, 0) > zscore(values, 0).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- All the true values are the outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values[96] = np.mean(values)\n",
    "values[36] = np.mean(values)\n",
    "# changing the outliers to mean value\n",
    "# handpicking abnormal values and setting it a justified value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 883,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: 5.397840428854952, Var: 3.2388358843830356, Std: 1.7996766054997313\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.scatterplot(data=values)\n",
    "print (\"Mean: {}, Var: {}, Std: {}\".format(values.mean(), values.var(), values.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We can see that the data is not too distributed and all values closely align."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Power_Level'] = values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 857,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated data of size:5000\n"
     ]
    }
   ],
   "source": [
    "def count_wins(df=None):\n",
    "    \"\"\"\n",
    "        Count number of character wins.\n",
    "        Returns:\n",
    "            c1_wins: Character 1 wins\n",
    "            c2_wins: Character 2 wins\n",
    "    \"\"\"\n",
    "    for i in range(0, len(df)):\n",
    "        if df.loc[i].character1 == df.loc[i].win:\n",
    "            c1_wins.append(i)\n",
    "        else:\n",
    "            c2_wins.append(i)\n",
    "    print(\"Character 1 wins: {}, Character 2 wins: {}\".format(len(c1_wins), len(c2_wins)))       \n",
    "    \n",
    "features, labels,x_train, x_test, y_train, y_test,x_val, y_val,data = create_data(df, size=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 786,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(max_iter=10000)"
      ]
     },
     "execution_count": 786,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 793,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(n_estimators=10000)"
      ]
     },
     "execution_count": 793,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf = RandomForestClassifier(n_estimators=10000)\n",
    "rf.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 795,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.355"
      ]
     },
     "execution_count": 795,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf.score(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 858,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Character 1 wins: 4037, Character 2 wins: 3963\n"
     ]
    }
   ],
   "source": [
    "count_wins(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 704,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "505"
      ]
     },
     "execution_count": 704,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(np.where(data.character1 == data.win)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 707,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-495"
      ]
     },
     "execution_count": 707,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 859,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "tensorboard = tf.keras.callbacks.TensorBoard(log_dir=\"./logs\")\n",
    "EarlyStopping = tf.keras.callbacks.EarlyStopping(patience=3)\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Dense(100, activation='relu', input_dim=10))\n",
    "model.add(tf.keras.layers.Dropout(0.2))\n",
    "model.add(tf.keras.layers.Dense(200, activation='relu'))\n",
    "model.add(tf.keras.layers.Dropout(0.5))\n",
    "model.add(tf.keras.layers.Dense(200, activation='relu'))\n",
    "model.add(tf.keras.layers.Dropout(0.5))\n",
    "model.add(tf.keras.layers.Dense(90, activation='softmax'))\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=3e-4), loss=tf.keras.losses.sparse_categorical_crossentropy, metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 860,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      " 2/94 [..............................] - ETA: 3s - loss: 17.3615 - acc: 0.0156WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0048s vs `on_train_batch_end` time: 0.0696s). Check your callbacks.\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 9.7800 - acc: 0.0113 - val_loss: 4.5090 - val_acc: 0.0110\n",
      "Epoch 2/300\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 5.5607 - acc: 0.0130 - val_loss: 4.4949 - val_acc: 0.0170\n",
      "Epoch 3/300\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 4.9805 - acc: 0.0100 - val_loss: 4.4935 - val_acc: 0.0210\n",
      "Epoch 4/300\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 4.7351 - acc: 0.0120 - val_loss: 4.4932 - val_acc: 0.0150\n",
      "Epoch 5/300\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 4.6627 - acc: 0.0147 - val_loss: 4.4960 - val_acc: 0.0130\n",
      "Epoch 6/300\n",
      "94/94 [==============================] - 1s 8ms/step - loss: 4.5986 - acc: 0.0127 - val_loss: 4.4959 - val_acc: 0.0130\n",
      "Epoch 7/300\n",
      "94/94 [==============================] - 0s 5ms/step - loss: 4.5918 - acc: 0.0197 - val_loss: 4.4960 - val_acc: 0.0100\n",
      "Epoch 8/300\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 4.5500 - acc: 0.0173 - val_loss: 4.4948 - val_acc: 0.0080\n",
      "Epoch 9/300\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 4.5371 - acc: 0.0133 - val_loss: 4.4931 - val_acc: 0.0120\n",
      "Epoch 10/300\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 4.5383 - acc: 0.0170 - val_loss: 4.4899 - val_acc: 0.0130\n",
      "Epoch 11/300\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 4.5150 - acc: 0.0187 - val_loss: 4.4827 - val_acc: 0.0140\n",
      "Epoch 12/300\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 4.4974 - acc: 0.0223 - val_loss: 4.4683 - val_acc: 0.0210\n",
      "Epoch 13/300\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 4.4898 - acc: 0.0180 - val_loss: 4.4545 - val_acc: 0.0170\n",
      "Epoch 14/300\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 4.4831 - acc: 0.0223 - val_loss: 4.4376 - val_acc: 0.0210\n",
      "Epoch 15/300\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 4.4729 - acc: 0.0207 - val_loss: 4.4191 - val_acc: 0.0180\n",
      "Epoch 16/300\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 4.4462 - acc: 0.0257 - val_loss: 4.4013 - val_acc: 0.0210\n",
      "Epoch 17/300\n",
      "94/94 [==============================] - 1s 7ms/step - loss: 4.4359 - acc: 0.0217 - val_loss: 4.3873 - val_acc: 0.0250\n",
      "Epoch 18/300\n",
      "94/94 [==============================] - 0s 5ms/step - loss: 4.4355 - acc: 0.0220 - val_loss: 4.3667 - val_acc: 0.0250\n",
      "Epoch 19/300\n",
      "94/94 [==============================] - 1s 5ms/step - loss: 4.4063 - acc: 0.0280 - val_loss: 4.3393 - val_acc: 0.0290\n",
      "Epoch 20/300\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 4.3858 - acc: 0.0273 - val_loss: 4.3247 - val_acc: 0.0280\n",
      "Epoch 21/300\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 4.3714 - acc: 0.0293 - val_loss: 4.3172 - val_acc: 0.0300\n",
      "Epoch 22/300\n",
      "94/94 [==============================] - 0s 5ms/step - loss: 4.3735 - acc: 0.0250 - val_loss: 4.3165 - val_acc: 0.0340\n",
      "Epoch 23/300\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 4.3560 - acc: 0.0260 - val_loss: 4.2964 - val_acc: 0.0260\n",
      "Epoch 24/300\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 4.3530 - acc: 0.0263 - val_loss: 4.2832 - val_acc: 0.0330\n",
      "Epoch 25/300\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 4.3236 - acc: 0.0280 - val_loss: 4.2703 - val_acc: 0.0330\n",
      "Epoch 26/300\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 4.3066 - acc: 0.0333 - val_loss: 4.2701 - val_acc: 0.0320\n",
      "Epoch 27/300\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 4.3080 - acc: 0.0333 - val_loss: 4.2570 - val_acc: 0.0320\n",
      "Epoch 28/300\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 4.3069 - acc: 0.0270 - val_loss: 4.2556 - val_acc: 0.0380\n",
      "Epoch 29/300\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 4.2925 - acc: 0.0300 - val_loss: 4.2432 - val_acc: 0.0390\n",
      "Epoch 30/300\n",
      "94/94 [==============================] - 0s 5ms/step - loss: 4.2804 - acc: 0.0347 - val_loss: 4.2300 - val_acc: 0.0410\n",
      "Epoch 31/300\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 4.2576 - acc: 0.0320 - val_loss: 4.2140 - val_acc: 0.0350\n",
      "Epoch 32/300\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 4.2400 - acc: 0.0400 - val_loss: 4.2074 - val_acc: 0.0400\n",
      "Epoch 33/300\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 4.2513 - acc: 0.0267 - val_loss: 4.1963 - val_acc: 0.0420\n",
      "Epoch 34/300\n",
      "94/94 [==============================] - 0s 5ms/step - loss: 4.2428 - acc: 0.0313 - val_loss: 4.1859 - val_acc: 0.0400\n",
      "Epoch 35/300\n",
      "94/94 [==============================] - 0s 5ms/step - loss: 4.2459 - acc: 0.0347 - val_loss: 4.1871 - val_acc: 0.0450\n",
      "Epoch 36/300\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 4.2195 - acc: 0.0387 - val_loss: 4.1681 - val_acc: 0.0430\n",
      "Epoch 37/300\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 4.1876 - acc: 0.0353 - val_loss: 4.1586 - val_acc: 0.0440\n",
      "Epoch 38/300\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 4.1859 - acc: 0.0377 - val_loss: 4.1447 - val_acc: 0.0410\n",
      "Epoch 39/300\n",
      "94/94 [==============================] - 0s 5ms/step - loss: 4.1834 - acc: 0.0357 - val_loss: 4.1362 - val_acc: 0.0470\n",
      "Epoch 40/300\n",
      "94/94 [==============================] - 0s 5ms/step - loss: 4.1751 - acc: 0.0337 - val_loss: 4.1269 - val_acc: 0.0470\n",
      "Epoch 41/300\n",
      "94/94 [==============================] - 0s 5ms/step - loss: 4.1687 - acc: 0.0353 - val_loss: 4.1209 - val_acc: 0.0480\n",
      "Epoch 42/300\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 4.1546 - acc: 0.0410 - val_loss: 4.1170 - val_acc: 0.0470\n",
      "Epoch 43/300\n",
      "94/94 [==============================] - 0s 5ms/step - loss: 4.1491 - acc: 0.0380 - val_loss: 4.0949 - val_acc: 0.0470\n",
      "Epoch 44/300\n",
      "94/94 [==============================] - 1s 7ms/step - loss: 4.1275 - acc: 0.0397 - val_loss: 4.0849 - val_acc: 0.0450\n",
      "Epoch 45/300\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 4.1364 - acc: 0.0397 - val_loss: 4.0755 - val_acc: 0.0520\n",
      "Epoch 46/300\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 4.1207 - acc: 0.0440 - val_loss: 4.0721 - val_acc: 0.0550\n",
      "Epoch 47/300\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 4.1036 - acc: 0.0487 - val_loss: 4.0572 - val_acc: 0.0510\n",
      "Epoch 48/300\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 4.0824 - acc: 0.0517 - val_loss: 4.0437 - val_acc: 0.0530\n",
      "Epoch 49/300\n",
      "94/94 [==============================] - 1s 8ms/step - loss: 4.0842 - acc: 0.0470 - val_loss: 4.0358 - val_acc: 0.0550\n",
      "Epoch 50/300\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 4.0717 - acc: 0.0467 - val_loss: 4.0163 - val_acc: 0.0580\n",
      "Epoch 51/300\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 4.0539 - acc: 0.0553 - val_loss: 4.0037 - val_acc: 0.0530\n",
      "Epoch 52/300\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 4.0442 - acc: 0.0517 - val_loss: 3.9942 - val_acc: 0.0570\n",
      "Epoch 53/300\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 4.0409 - acc: 0.0473 - val_loss: 3.9783 - val_acc: 0.0590\n",
      "Epoch 54/300\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 4.0252 - acc: 0.0430 - val_loss: 3.9713 - val_acc: 0.0650\n",
      "Epoch 55/300\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 4.0097 - acc: 0.0493 - val_loss: 3.9592 - val_acc: 0.0600\n",
      "Epoch 56/300\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 4.0007 - acc: 0.0517 - val_loss: 3.9459 - val_acc: 0.0610\n",
      "Epoch 57/300\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 3.9907 - acc: 0.0427 - val_loss: 3.9325 - val_acc: 0.0680\n",
      "Epoch 58/300\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 3.9751 - acc: 0.0523 - val_loss: 3.9183 - val_acc: 0.0590\n",
      "Epoch 59/300\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 3.9557 - acc: 0.0563 - val_loss: 3.9074 - val_acc: 0.0710\n",
      "Epoch 60/300\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 3.9691 - acc: 0.0453 - val_loss: 3.9048 - val_acc: 0.0730\n",
      "Epoch 61/300\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 3.9671 - acc: 0.0533 - val_loss: 3.8981 - val_acc: 0.0740\n",
      "Epoch 62/300\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 3.9398 - acc: 0.0487 - val_loss: 3.8934 - val_acc: 0.0740\n",
      "Epoch 63/300\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 3.9381 - acc: 0.0527 - val_loss: 3.8993 - val_acc: 0.0660\n",
      "Epoch 64/300\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 3.9167 - acc: 0.0537 - val_loss: 3.8753 - val_acc: 0.0710\n",
      "Epoch 65/300\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 3.9127 - acc: 0.0600 - val_loss: 3.8679 - val_acc: 0.0690\n",
      "Epoch 66/300\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 3.9160 - acc: 0.0577 - val_loss: 3.8615 - val_acc: 0.0710\n",
      "Epoch 67/300\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 3.8917 - acc: 0.0617 - val_loss: 3.8435 - val_acc: 0.0800\n",
      "Epoch 68/300\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 3.8927 - acc: 0.0610 - val_loss: 3.8439 - val_acc: 0.0730\n",
      "Epoch 69/300\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 3.8878 - acc: 0.0600 - val_loss: 3.8424 - val_acc: 0.0770\n",
      "Epoch 70/300\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 3.8789 - acc: 0.0657 - val_loss: 3.8274 - val_acc: 0.0820\n",
      "Epoch 71/300\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 3.8672 - acc: 0.0617 - val_loss: 3.8165 - val_acc: 0.0800\n",
      "Epoch 72/300\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 3.8490 - acc: 0.0657 - val_loss: 3.8112 - val_acc: 0.0770\n",
      "Epoch 73/300\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 3.8497 - acc: 0.0660 - val_loss: 3.8126 - val_acc: 0.0760\n",
      "Epoch 74/300\n",
      "94/94 [==============================] - 1s 9ms/step - loss: 3.8461 - acc: 0.0650 - val_loss: 3.8019 - val_acc: 0.0760\n",
      "Epoch 75/300\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 3.8295 - acc: 0.0633 - val_loss: 3.7994 - val_acc: 0.0760\n",
      "Epoch 76/300\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 3.8218 - acc: 0.0663 - val_loss: 3.7844 - val_acc: 0.0790\n",
      "Epoch 77/300\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 3.8122 - acc: 0.0740 - val_loss: 3.7676 - val_acc: 0.0910\n",
      "Epoch 78/300\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 3.7932 - acc: 0.0787 - val_loss: 3.7615 - val_acc: 0.0860\n",
      "Epoch 79/300\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 3.7978 - acc: 0.0650 - val_loss: 3.7570 - val_acc: 0.0800\n",
      "Epoch 80/300\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 3.7933 - acc: 0.0720 - val_loss: 3.7492 - val_acc: 0.0930\n",
      "Epoch 81/300\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 3.7746 - acc: 0.0663 - val_loss: 3.7389 - val_acc: 0.0960\n",
      "Epoch 82/300\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 3.7643 - acc: 0.0690 - val_loss: 3.7357 - val_acc: 0.0960\n",
      "Epoch 83/300\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 3.7450 - acc: 0.0693 - val_loss: 3.7153 - val_acc: 0.0970\n",
      "Epoch 84/300\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 3.7497 - acc: 0.0770 - val_loss: 3.7076 - val_acc: 0.0970\n",
      "Epoch 85/300\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 3.7290 - acc: 0.0710 - val_loss: 3.6998 - val_acc: 0.0980\n",
      "Epoch 86/300\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 3.7388 - acc: 0.0723 - val_loss: 3.6884 - val_acc: 0.1040\n",
      "Epoch 87/300\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 3.7214 - acc: 0.0793 - val_loss: 3.6834 - val_acc: 0.1020\n",
      "Epoch 88/300\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 3.7025 - acc: 0.0823 - val_loss: 3.6710 - val_acc: 0.1020\n",
      "Epoch 89/300\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 3.6847 - acc: 0.0830 - val_loss: 3.6652 - val_acc: 0.1010\n",
      "Epoch 90/300\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 3.6976 - acc: 0.0783 - val_loss: 3.6464 - val_acc: 0.1150\n",
      "Epoch 91/300\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 3.6795 - acc: 0.0790 - val_loss: 3.6394 - val_acc: 0.1050\n",
      "Epoch 92/300\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 3.6693 - acc: 0.0810 - val_loss: 3.6286 - val_acc: 0.1130\n",
      "Epoch 93/300\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 3.6612 - acc: 0.0860 - val_loss: 3.6225 - val_acc: 0.1080\n",
      "Epoch 94/300\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 3.6505 - acc: 0.0817 - val_loss: 3.6236 - val_acc: 0.1030\n",
      "Epoch 95/300\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 3.6513 - acc: 0.0807 - val_loss: 3.6111 - val_acc: 0.0960\n",
      "Epoch 96/300\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 3.6413 - acc: 0.0840 - val_loss: 3.5985 - val_acc: 0.1030\n",
      "Epoch 97/300\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 3.6268 - acc: 0.0897 - val_loss: 3.5874 - val_acc: 0.1130\n",
      "Epoch 98/300\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 3.6245 - acc: 0.0870 - val_loss: 3.5909 - val_acc: 0.1080\n",
      "Epoch 99/300\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 3.6094 - acc: 0.0947 - val_loss: 3.5821 - val_acc: 0.1050\n",
      "Epoch 100/300\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 3.6017 - acc: 0.0917 - val_loss: 3.5755 - val_acc: 0.1110\n",
      "Epoch 101/300\n",
      "94/94 [==============================] - 0s 5ms/step - loss: 3.5821 - acc: 0.0983 - val_loss: 3.5527 - val_acc: 0.1130\n",
      "Epoch 102/300\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 3.5938 - acc: 0.0897 - val_loss: 3.5545 - val_acc: 0.1110\n",
      "Epoch 103/300\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 3.5670 - acc: 0.1060 - val_loss: 3.5384 - val_acc: 0.1160\n",
      "Epoch 104/300\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 3.5551 - acc: 0.1043 - val_loss: 3.5318 - val_acc: 0.1200\n",
      "Epoch 105/300\n",
      "94/94 [==============================] - 0s 5ms/step - loss: 3.5529 - acc: 0.0983 - val_loss: 3.5208 - val_acc: 0.1230\n",
      "Epoch 106/300\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 3.5334 - acc: 0.0950 - val_loss: 3.5160 - val_acc: 0.1270\n",
      "Epoch 107/300\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 3.5297 - acc: 0.0953 - val_loss: 3.5119 - val_acc: 0.1230\n",
      "Epoch 108/300\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 3.5234 - acc: 0.0983 - val_loss: 3.5012 - val_acc: 0.1340\n",
      "Epoch 109/300\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 3.5282 - acc: 0.0967 - val_loss: 3.5028 - val_acc: 0.1270\n",
      "Epoch 110/300\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 3.5222 - acc: 0.0980 - val_loss: 3.4884 - val_acc: 0.1320\n",
      "Epoch 111/300\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 3.5271 - acc: 0.1007 - val_loss: 3.4838 - val_acc: 0.1300\n",
      "Epoch 112/300\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 3.5156 - acc: 0.1023 - val_loss: 3.4755 - val_acc: 0.1360\n",
      "Epoch 113/300\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 3.4947 - acc: 0.1017 - val_loss: 3.4826 - val_acc: 0.1370\n",
      "Epoch 114/300\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 3.4662 - acc: 0.1083 - val_loss: 3.4545 - val_acc: 0.1330\n",
      "Epoch 115/300\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 3.4913 - acc: 0.0997 - val_loss: 3.4481 - val_acc: 0.1280\n",
      "Epoch 116/300\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 3.4656 - acc: 0.1093 - val_loss: 3.4404 - val_acc: 0.1370\n",
      "Epoch 117/300\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 3.4611 - acc: 0.1070 - val_loss: 3.4377 - val_acc: 0.1450\n",
      "Epoch 118/300\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 3.4493 - acc: 0.1030 - val_loss: 3.4350 - val_acc: 0.1400\n",
      "Epoch 119/300\n",
      "94/94 [==============================] - 0s 5ms/step - loss: 3.4446 - acc: 0.1087 - val_loss: 3.4223 - val_acc: 0.1410\n",
      "Epoch 120/300\n",
      "94/94 [==============================] - 0s 5ms/step - loss: 3.4316 - acc: 0.1167 - val_loss: 3.4235 - val_acc: 0.1370\n",
      "Epoch 121/300\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 3.4271 - acc: 0.1130 - val_loss: 3.4074 - val_acc: 0.1440\n",
      "Epoch 122/300\n",
      "94/94 [==============================] - 0s 5ms/step - loss: 3.4273 - acc: 0.1143 - val_loss: 3.4051 - val_acc: 0.1410\n",
      "Epoch 123/300\n",
      "94/94 [==============================] - 1s 5ms/step - loss: 3.4166 - acc: 0.1160 - val_loss: 3.3979 - val_acc: 0.1450\n",
      "Epoch 124/300\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 3.3856 - acc: 0.1133 - val_loss: 3.3861 - val_acc: 0.1380\n",
      "Epoch 125/300\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 3.3940 - acc: 0.1167 - val_loss: 3.3776 - val_acc: 0.1400\n",
      "Epoch 126/300\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 3.3918 - acc: 0.1207 - val_loss: 3.3692 - val_acc: 0.1390\n",
      "Epoch 127/300\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 3.3634 - acc: 0.1240 - val_loss: 3.3646 - val_acc: 0.1350\n",
      "Epoch 128/300\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 3.3823 - acc: 0.1237 - val_loss: 3.3594 - val_acc: 0.1370\n",
      "Epoch 129/300\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 3.3763 - acc: 0.1243 - val_loss: 3.3548 - val_acc: 0.1470\n",
      "Epoch 130/300\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 3.3687 - acc: 0.1233 - val_loss: 3.3471 - val_acc: 0.1490\n",
      "Epoch 131/300\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 3.3551 - acc: 0.1210 - val_loss: 3.3391 - val_acc: 0.1490\n",
      "Epoch 132/300\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 3.3545 - acc: 0.1223 - val_loss: 3.3306 - val_acc: 0.1570\n",
      "Epoch 133/300\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 3.3515 - acc: 0.1227 - val_loss: 3.3311 - val_acc: 0.1490\n",
      "Epoch 134/300\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 3.3140 - acc: 0.1323 - val_loss: 3.3142 - val_acc: 0.1640\n",
      "Epoch 135/300\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 3.3042 - acc: 0.1323 - val_loss: 3.3096 - val_acc: 0.1490\n",
      "Epoch 136/300\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 3.3073 - acc: 0.1360 - val_loss: 3.2995 - val_acc: 0.1540\n",
      "Epoch 137/300\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 3.3061 - acc: 0.1313 - val_loss: 3.2885 - val_acc: 0.1700\n",
      "Epoch 138/300\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 3.2957 - acc: 0.1333 - val_loss: 3.2895 - val_acc: 0.1510\n",
      "Epoch 139/300\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 3.3077 - acc: 0.1323 - val_loss: 3.2782 - val_acc: 0.1620\n",
      "Epoch 140/300\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 3.2956 - acc: 0.1267 - val_loss: 3.2721 - val_acc: 0.1600\n",
      "Epoch 141/300\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 3.2976 - acc: 0.1307 - val_loss: 3.2619 - val_acc: 0.1630\n",
      "Epoch 142/300\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 3.3069 - acc: 0.1310 - val_loss: 3.2742 - val_acc: 0.1710\n",
      "Epoch 143/300\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 3.2870 - acc: 0.1343 - val_loss: 3.2500 - val_acc: 0.1690\n",
      "Epoch 144/300\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 3.2886 - acc: 0.1380 - val_loss: 3.2483 - val_acc: 0.1550\n",
      "Epoch 145/300\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 3.2732 - acc: 0.1357 - val_loss: 3.2567 - val_acc: 0.1570\n",
      "Epoch 146/300\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 3.2543 - acc: 0.1317 - val_loss: 3.2356 - val_acc: 0.1620\n",
      "Epoch 147/300\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 3.2456 - acc: 0.1430 - val_loss: 3.2369 - val_acc: 0.1540\n",
      "Epoch 148/300\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 3.2522 - acc: 0.1390 - val_loss: 3.2340 - val_acc: 0.1630\n",
      "Epoch 149/300\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 3.2468 - acc: 0.1387 - val_loss: 3.2213 - val_acc: 0.1650\n",
      "Epoch 150/300\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 3.2189 - acc: 0.1463 - val_loss: 3.2233 - val_acc: 0.1560\n",
      "Epoch 151/300\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 3.2300 - acc: 0.1353 - val_loss: 3.2161 - val_acc: 0.1580\n",
      "Epoch 152/300\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 3.2256 - acc: 0.1447 - val_loss: 3.2040 - val_acc: 0.1590\n",
      "Epoch 153/300\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 3.2117 - acc: 0.1493 - val_loss: 3.1991 - val_acc: 0.1670\n",
      "Epoch 154/300\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 3.2040 - acc: 0.1570 - val_loss: 3.1913 - val_acc: 0.1620\n",
      "Epoch 155/300\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 3.2003 - acc: 0.1427 - val_loss: 3.1933 - val_acc: 0.1630\n",
      "Epoch 156/300\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 3.1929 - acc: 0.1473 - val_loss: 3.1906 - val_acc: 0.1650\n",
      "Epoch 157/300\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 3.1494 - acc: 0.1530 - val_loss: 3.1686 - val_acc: 0.1680\n",
      "Epoch 158/300\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 3.1795 - acc: 0.1507 - val_loss: 3.1794 - val_acc: 0.1590\n",
      "Epoch 159/300\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 3.1764 - acc: 0.1473 - val_loss: 3.1707 - val_acc: 0.1680\n",
      "Epoch 160/300\n",
      "94/94 [==============================] - ETA: 0s - loss: 3.1965 - acc: 0.143 - 0s 3ms/step - loss: 3.1933 - acc: 0.1427 - val_loss: 3.1632 - val_acc: 0.1720\n",
      "Epoch 161/300\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 3.1578 - acc: 0.1640 - val_loss: 3.1605 - val_acc: 0.1720\n",
      "Epoch 162/300\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 3.1718 - acc: 0.1523 - val_loss: 3.1556 - val_acc: 0.1700\n",
      "Epoch 163/300\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 3.1411 - acc: 0.1600 - val_loss: 3.1463 - val_acc: 0.1740\n",
      "Epoch 164/300\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 3.1524 - acc: 0.1450 - val_loss: 3.1520 - val_acc: 0.1860\n",
      "Epoch 165/300\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 3.1344 - acc: 0.1550 - val_loss: 3.1406 - val_acc: 0.1740\n",
      "Epoch 166/300\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 3.1632 - acc: 0.1583 - val_loss: 3.1276 - val_acc: 0.1780\n",
      "Epoch 167/300\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 3.1343 - acc: 0.1520 - val_loss: 3.1274 - val_acc: 0.1730\n",
      "Epoch 168/300\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 3.1381 - acc: 0.1627 - val_loss: 3.1219 - val_acc: 0.1840\n",
      "Epoch 169/300\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 3.1088 - acc: 0.1557 - val_loss: 3.1118 - val_acc: 0.1770\n",
      "Epoch 170/300\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 3.1187 - acc: 0.1570 - val_loss: 3.1126 - val_acc: 0.1850\n",
      "Epoch 171/300\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 3.1207 - acc: 0.1553 - val_loss: 3.1108 - val_acc: 0.1780\n",
      "Epoch 172/300\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 3.1037 - acc: 0.1560 - val_loss: 3.0979 - val_acc: 0.1850\n",
      "Epoch 173/300\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 3.1152 - acc: 0.1627 - val_loss: 3.1148 - val_acc: 0.1820\n",
      "Epoch 174/300\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 3.0710 - acc: 0.1680 - val_loss: 3.0914 - val_acc: 0.1880\n",
      "Epoch 175/300\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 3.0831 - acc: 0.1660 - val_loss: 3.0916 - val_acc: 0.1920\n",
      "Epoch 176/300\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 3.0872 - acc: 0.1697 - val_loss: 3.1018 - val_acc: 0.1860\n",
      "Epoch 177/300\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 3.0925 - acc: 0.1513 - val_loss: 3.0783 - val_acc: 0.1880\n",
      "Epoch 178/300\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 3.0595 - acc: 0.1713 - val_loss: 3.0722 - val_acc: 0.1960\n",
      "Epoch 179/300\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 3.0738 - acc: 0.1673 - val_loss: 3.0892 - val_acc: 0.1870\n",
      "Epoch 180/300\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 3.0614 - acc: 0.1743 - val_loss: 3.0588 - val_acc: 0.2000\n",
      "Epoch 181/300\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 3.0677 - acc: 0.1640 - val_loss: 3.0569 - val_acc: 0.1980\n",
      "Epoch 182/300\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 3.0453 - acc: 0.1723 - val_loss: 3.0807 - val_acc: 0.1880\n",
      "Epoch 183/300\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 3.0526 - acc: 0.1800 - val_loss: 3.0579 - val_acc: 0.2070\n",
      "Epoch 184/300\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 3.0498 - acc: 0.1633 - val_loss: 3.0485 - val_acc: 0.1960\n",
      "Epoch 185/300\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 3.0610 - acc: 0.1713 - val_loss: 3.0625 - val_acc: 0.1880\n",
      "Epoch 186/300\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 3.0225 - acc: 0.1653 - val_loss: 3.0366 - val_acc: 0.1950\n",
      "Epoch 187/300\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 3.0457 - acc: 0.1843 - val_loss: 3.0476 - val_acc: 0.1960\n",
      "Epoch 188/300\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 3.0272 - acc: 0.1590 - val_loss: 3.0401 - val_acc: 0.2030\n",
      "Epoch 189/300\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 3.0147 - acc: 0.1737 - val_loss: 3.0308 - val_acc: 0.1910\n",
      "Epoch 190/300\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 3.0280 - acc: 0.1730 - val_loss: 3.0174 - val_acc: 0.2060\n",
      "Epoch 191/300\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 3.0148 - acc: 0.1763 - val_loss: 3.0122 - val_acc: 0.2070\n",
      "Epoch 192/300\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 3.0012 - acc: 0.1870 - val_loss: 3.0035 - val_acc: 0.2110\n",
      "Epoch 193/300\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 3.0437 - acc: 0.1733 - val_loss: 3.0265 - val_acc: 0.1970\n",
      "Epoch 194/300\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 2.9959 - acc: 0.1737 - val_loss: 3.0066 - val_acc: 0.2100\n",
      "Epoch 195/300\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 3.0048 - acc: 0.1813 - val_loss: 2.9958 - val_acc: 0.2090\n",
      "Epoch 196/300\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 2.9924 - acc: 0.1810 - val_loss: 2.9961 - val_acc: 0.2140\n",
      "Epoch 197/300\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 3.0040 - acc: 0.1813 - val_loss: 2.9974 - val_acc: 0.2010\n",
      "Epoch 198/300\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 2.9647 - acc: 0.1740 - val_loss: 2.9956 - val_acc: 0.2000\n",
      "Epoch 199/300\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 2.9889 - acc: 0.1767 - val_loss: 2.9765 - val_acc: 0.2150\n",
      "Epoch 200/300\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 2.9899 - acc: 0.1797 - val_loss: 2.9736 - val_acc: 0.2170\n",
      "Epoch 201/300\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 2.9789 - acc: 0.1783 - val_loss: 2.9860 - val_acc: 0.2090\n",
      "Epoch 202/300\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 2.9776 - acc: 0.1770 - val_loss: 2.9582 - val_acc: 0.2070\n",
      "Epoch 203/300\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 2.9324 - acc: 0.1997 - val_loss: 2.9558 - val_acc: 0.2100\n",
      "Epoch 204/300\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 2.9643 - acc: 0.1833 - val_loss: 2.9704 - val_acc: 0.2120\n",
      "Epoch 205/300\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 2.9484 - acc: 0.1823 - val_loss: 2.9818 - val_acc: 0.2020\n",
      "Epoch 206/300\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 2.9638 - acc: 0.1830 - val_loss: 2.9561 - val_acc: 0.2120\n",
      "Epoch 207/300\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 2.9406 - acc: 0.1993 - val_loss: 2.9603 - val_acc: 0.2070\n",
      "Epoch 208/300\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 2.9586 - acc: 0.1783 - val_loss: 2.9693 - val_acc: 0.1970\n",
      "Epoch 209/300\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 2.9334 - acc: 0.1960 - val_loss: 2.9441 - val_acc: 0.2110\n",
      "Epoch 210/300\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 2.9401 - acc: 0.1827 - val_loss: 2.9383 - val_acc: 0.2180\n",
      "Epoch 211/300\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 2.9390 - acc: 0.1903 - val_loss: 2.9587 - val_acc: 0.2080\n",
      "Epoch 212/300\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 2.8951 - acc: 0.2010 - val_loss: 2.9386 - val_acc: 0.2220\n",
      "Epoch 213/300\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 2.9214 - acc: 0.1917 - val_loss: 2.9432 - val_acc: 0.2210\n",
      "Epoch 214/300\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 2.9230 - acc: 0.1890 - val_loss: 2.9195 - val_acc: 0.2190\n",
      "Epoch 215/300\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 2.9031 - acc: 0.1887 - val_loss: 2.9177 - val_acc: 0.2180\n",
      "Epoch 216/300\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 2.9268 - acc: 0.1867 - val_loss: 2.9047 - val_acc: 0.2220\n",
      "Epoch 217/300\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 2.9120 - acc: 0.1870 - val_loss: 2.9162 - val_acc: 0.2290\n",
      "Epoch 218/300\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 2.9169 - acc: 0.1897 - val_loss: 2.9247 - val_acc: 0.2110\n",
      "Epoch 219/300\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 2.8890 - acc: 0.1967 - val_loss: 2.9277 - val_acc: 0.2180\n",
      "Epoch 220/300\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 2.8970 - acc: 0.1923 - val_loss: 2.9137 - val_acc: 0.2260\n",
      "Epoch 221/300\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 2.8652 - acc: 0.1903 - val_loss: 2.9174 - val_acc: 0.2210\n",
      "Epoch 222/300\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 2.8991 - acc: 0.1877 - val_loss: 2.9262 - val_acc: 0.2190\n",
      "Epoch 223/300\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 2.8641 - acc: 0.1957 - val_loss: 2.9029 - val_acc: 0.2260\n",
      "Epoch 224/300\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 2.8894 - acc: 0.1957 - val_loss: 2.8904 - val_acc: 0.2320\n",
      "Epoch 225/300\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 2.8641 - acc: 0.2053 - val_loss: 2.9015 - val_acc: 0.2320\n",
      "Epoch 226/300\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 2.8857 - acc: 0.1913 - val_loss: 2.8978 - val_acc: 0.2220\n",
      "Epoch 227/300\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 2.8567 - acc: 0.2003 - val_loss: 2.8829 - val_acc: 0.2350\n",
      "Epoch 228/300\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 2.8537 - acc: 0.1997 - val_loss: 2.9045 - val_acc: 0.2220\n",
      "Epoch 229/300\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 2.8626 - acc: 0.2023 - val_loss: 2.8676 - val_acc: 0.2390\n",
      "Epoch 230/300\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 2.8511 - acc: 0.1983 - val_loss: 2.8840 - val_acc: 0.2230\n",
      "Epoch 231/300\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 2.8491 - acc: 0.1993 - val_loss: 2.8647 - val_acc: 0.2380\n",
      "Epoch 232/300\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 2.8807 - acc: 0.1937 - val_loss: 2.8779 - val_acc: 0.2290\n",
      "Epoch 233/300\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 2.8538 - acc: 0.1947 - val_loss: 2.8751 - val_acc: 0.2270\n",
      "Epoch 234/300\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 2.8376 - acc: 0.2013 - val_loss: 2.8672 - val_acc: 0.2290\n",
      "Epoch 235/300\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 2.8541 - acc: 0.2047 - val_loss: 2.8463 - val_acc: 0.2380\n",
      "Epoch 236/300\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 2.8383 - acc: 0.1907 - val_loss: 2.8443 - val_acc: 0.2420\n",
      "Epoch 237/300\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 2.8289 - acc: 0.2097 - val_loss: 2.8390 - val_acc: 0.2380\n",
      "Epoch 238/300\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 2.8410 - acc: 0.1917 - val_loss: 2.8541 - val_acc: 0.2340\n",
      "Epoch 239/300\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 2.8292 - acc: 0.2067 - val_loss: 2.8496 - val_acc: 0.2420\n",
      "Epoch 240/300\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 2.8012 - acc: 0.1967 - val_loss: 2.8375 - val_acc: 0.2460\n",
      "Epoch 241/300\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 2.8001 - acc: 0.2130 - val_loss: 2.8471 - val_acc: 0.2350\n",
      "Epoch 242/300\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 2.8098 - acc: 0.2063 - val_loss: 2.8309 - val_acc: 0.2330\n",
      "Epoch 243/300\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 2.8326 - acc: 0.1960 - val_loss: 2.8576 - val_acc: 0.2270\n",
      "Epoch 244/300\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 2.8416 - acc: 0.1977 - val_loss: 2.8255 - val_acc: 0.2500\n",
      "Epoch 245/300\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 2.8014 - acc: 0.1993 - val_loss: 2.8354 - val_acc: 0.2490\n",
      "Epoch 246/300\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 2.7652 - acc: 0.2160 - val_loss: 2.8196 - val_acc: 0.2480\n",
      "Epoch 247/300\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 2.8062 - acc: 0.1950 - val_loss: 2.8170 - val_acc: 0.2470\n",
      "Epoch 248/300\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 2.7960 - acc: 0.2063 - val_loss: 2.8157 - val_acc: 0.2400\n",
      "Epoch 249/300\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 2.7962 - acc: 0.2077 - val_loss: 2.8072 - val_acc: 0.2480\n",
      "Epoch 250/300\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 2.7760 - acc: 0.2033 - val_loss: 2.8195 - val_acc: 0.2390\n",
      "Epoch 251/300\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 2.7898 - acc: 0.2093 - val_loss: 2.8198 - val_acc: 0.2420\n",
      "Epoch 252/300\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 2.7704 - acc: 0.2113 - val_loss: 2.7940 - val_acc: 0.2530\n",
      "Epoch 253/300\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 2.7796 - acc: 0.2137 - val_loss: 2.7935 - val_acc: 0.2420\n",
      "Epoch 254/300\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 2.7863 - acc: 0.2097 - val_loss: 2.8092 - val_acc: 0.2420\n",
      "Epoch 255/300\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 2.7620 - acc: 0.2253 - val_loss: 2.7883 - val_acc: 0.2450\n",
      "Epoch 256/300\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 2.7807 - acc: 0.2137 - val_loss: 2.8101 - val_acc: 0.2430\n",
      "Epoch 257/300\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 2.7544 - acc: 0.2160 - val_loss: 2.7908 - val_acc: 0.2430\n",
      "Epoch 258/300\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 2.7731 - acc: 0.2110 - val_loss: 2.7726 - val_acc: 0.2470\n",
      "Epoch 259/300\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 2.7758 - acc: 0.2043 - val_loss: 2.7769 - val_acc: 0.2530\n",
      "Epoch 260/300\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 2.7494 - acc: 0.2137 - val_loss: 2.7890 - val_acc: 0.2480\n",
      "Epoch 261/300\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 2.7559 - acc: 0.2047 - val_loss: 2.7783 - val_acc: 0.2550\n",
      "Epoch 262/300\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 2.7248 - acc: 0.2213 - val_loss: 2.7707 - val_acc: 0.2590\n",
      "Epoch 263/300\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 2.7395 - acc: 0.2110 - val_loss: 2.7711 - val_acc: 0.2580\n",
      "Epoch 264/300\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 2.7443 - acc: 0.2143 - val_loss: 2.7764 - val_acc: 0.2320\n",
      "Epoch 265/300\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 2.7393 - acc: 0.2197 - val_loss: 2.7759 - val_acc: 0.2460\n",
      "Epoch 266/300\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 2.7264 - acc: 0.2237 - val_loss: 2.7754 - val_acc: 0.2480\n",
      "Epoch 267/300\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 2.7532 - acc: 0.2137 - val_loss: 2.7755 - val_acc: 0.2510\n",
      "Epoch 268/300\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 2.7541 - acc: 0.2233 - val_loss: 2.7901 - val_acc: 0.2470\n",
      "Epoch 269/300\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 2.7478 - acc: 0.2140 - val_loss: 2.7621 - val_acc: 0.2510\n",
      "Epoch 270/300\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 2.7160 - acc: 0.2190 - val_loss: 2.7477 - val_acc: 0.2490\n",
      "Epoch 271/300\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 2.7217 - acc: 0.2270 - val_loss: 2.7586 - val_acc: 0.2530\n",
      "Epoch 272/300\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 2.7448 - acc: 0.2157 - val_loss: 2.7651 - val_acc: 0.2480\n",
      "Epoch 273/300\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 2.7360 - acc: 0.2207 - val_loss: 2.7624 - val_acc: 0.2460\n",
      "Epoch 274/300\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 2.7120 - acc: 0.2193 - val_loss: 2.7540 - val_acc: 0.2610\n",
      "Epoch 275/300\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 2.7127 - acc: 0.2223 - val_loss: 2.7499 - val_acc: 0.2590\n",
      "Epoch 276/300\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 2.7087 - acc: 0.2183 - val_loss: 2.7434 - val_acc: 0.2560\n",
      "Epoch 277/300\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 2.7501 - acc: 0.2113 - val_loss: 2.7433 - val_acc: 0.2670\n",
      "Epoch 278/300\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 2.7115 - acc: 0.2230 - val_loss: 2.7608 - val_acc: 0.2560\n",
      "Epoch 279/300\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 2.7069 - acc: 0.2227 - val_loss: 2.7352 - val_acc: 0.2620\n",
      "Epoch 280/300\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 2.7126 - acc: 0.2100 - val_loss: 2.7421 - val_acc: 0.2670\n",
      "Epoch 281/300\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 2.7122 - acc: 0.2317 - val_loss: 2.7223 - val_acc: 0.2640\n",
      "Epoch 282/300\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 2.6852 - acc: 0.2290 - val_loss: 2.7454 - val_acc: 0.2650\n",
      "Epoch 283/300\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 2.6933 - acc: 0.2293 - val_loss: 2.7334 - val_acc: 0.2610\n",
      "Epoch 284/300\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 2.6882 - acc: 0.2217 - val_loss: 2.7240 - val_acc: 0.2670\n",
      "Epoch 285/300\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 2.6702 - acc: 0.2390 - val_loss: 2.7208 - val_acc: 0.2640\n",
      "Epoch 286/300\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 2.6796 - acc: 0.2247 - val_loss: 2.7193 - val_acc: 0.2590\n",
      "Epoch 287/300\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 2.6964 - acc: 0.2203 - val_loss: 2.7319 - val_acc: 0.2620\n",
      "Epoch 288/300\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 2.6847 - acc: 0.2230 - val_loss: 2.7167 - val_acc: 0.2580\n",
      "Epoch 289/300\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 2.6765 - acc: 0.2277 - val_loss: 2.7237 - val_acc: 0.2630\n",
      "Epoch 290/300\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 2.6571 - acc: 0.2337 - val_loss: 2.7062 - val_acc: 0.2600\n",
      "Epoch 291/300\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 2.6918 - acc: 0.2153 - val_loss: 2.7062 - val_acc: 0.2750\n",
      "Epoch 292/300\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 2.6664 - acc: 0.2240 - val_loss: 2.7067 - val_acc: 0.2670\n",
      "Epoch 293/300\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 2.6706 - acc: 0.2250 - val_loss: 2.6931 - val_acc: 0.2710\n",
      "Epoch 294/300\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 2.6925 - acc: 0.2237 - val_loss: 2.7019 - val_acc: 0.2660\n",
      "Epoch 295/300\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 2.6494 - acc: 0.2390 - val_loss: 2.7069 - val_acc: 0.2560\n",
      "Epoch 296/300\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 2.6581 - acc: 0.2173 - val_loss: 2.7025 - val_acc: 0.2770\n",
      "Epoch 297/300\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 2.6747 - acc: 0.2287 - val_loss: 2.6988 - val_acc: 0.2680\n",
      "Epoch 298/300\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 2.6570 - acc: 0.2337 - val_loss: 2.6865 - val_acc: 0.2660\n",
      "Epoch 299/300\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 2.6764 - acc: 0.2243 - val_loss: 2.7004 - val_acc: 0.2600\n",
      "Epoch 300/300\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 2.6692 - acc: 0.2337 - val_loss: 2.6995 - val_acc: 0.2660\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train, y_train, epochs=300, validation_data=(x_val, y_val), callbacks=[tensorboard])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 828,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.345"
      ]
     },
     "execution_count": 828,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf.score(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 890,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_36\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_114 (Dense)            (None, 100)               1100      \n",
      "_________________________________________________________________\n",
      "dropout_51 (Dropout)         (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_115 (Dense)            (None, 200)               20200     \n",
      "_________________________________________________________________\n",
      "dropout_52 (Dropout)         (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_116 (Dense)            (None, 200)               40200     \n",
      "_________________________________________________________________\n",
      "dropout_53 (Dropout)         (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_117 (Dense)            (None, 90)                18090     \n",
      "=================================================================\n",
      "Total params: 79,590\n",
      "Trainable params: 79,590\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 861,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_history(history):\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'val'], loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "epochs = range(1, 301)\n",
    "\n",
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 863,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AGGREGOR -> 4.203285397692824e-13\n",
      "ALIEN X -> 1.5128327193400515e-12\n",
      "AMPFIBIAN -> 5.55081823807777e-15\n",
      "ARMODRILLO -> 5.7059777042178794e-11\n",
      "ARTIGUANA -> 3.1705682523863743e-09\n",
      "ASTRODACTYLE -> 1.996169939413872e-12\n",
      "ATOMIC X -> 6.816132014364484e-08\n",
      "ATOMIX -> 1.3011019291297998e-05\n",
      "BALL WEEVIL -> 4.928316360965166e-15\n",
      "BEN TEN THOUSAND -> 1.1854661696730773e-10\n",
      "BEN TENNYSON -> 1.581889509655451e-15\n",
      "BIG CHILL -> 1.2278659596631769e-05\n",
      "BLOXX (INDESTRUCTIBLE) -> 2.0889871166218654e-07\n",
      "BRAINSTORM -> 3.942162948078476e-06\n",
      "BULLFRAG -> 0.0008464688435196877\n",
      "BUZZSHOCK -> 2.6531442927080207e-05\n",
      "CANNONBOLT -> 0.011027975007891655\n",
      "CHARMCASTER -> 0.0056919618509709835\n",
      "CHROMOSTONE -> 0.02730976790189743\n",
      "CRASHHOPPER -> 0.00017758151807356626\n",
      "DARKSTAR -> 0.0004205204895697534\n",
      "DIAGON -> 0.07080186158418655\n",
      "DIAMONDHEAD -> 0.07565552741289139\n",
      "DNALIEN -> 6.342388303437474e-08\n",
      "ECHOECHO -> 4.924354470858816e-06\n",
      "EON -> 0.31206458806991577\n",
      "EON'S MINIONS -> 7.751674996336888e-11\n",
      "FEEDBACK -> 0.0021991869434714317\n",
      "FOURARMS -> 1.5034942180136568e-06\n",
      "FOURMUNGOUSAUR -> 0.004123568069189787\n",
      "GHOSTFREAK -> 1.7536714949528687e-05\n",
      "GOOP -> 2.6113261242244334e-07\n",
      "GRANDPA MAX -> 5.396021407477747e-10\n",
      "GRAVATTACK -> 0.0005607054918073118\n",
      "GREYMATTER -> 1.0584157113030165e-13\n",
      "GUTROT -> 7.81832350185141e-05\n",
      "GWEN -> 5.206453010941914e-07\n",
      "GWEN (MAGIC) -> 2.570122781264672e-09\n",
      "GWEN FULL ANODITE -> 0.003811942646279931\n",
      "GWEN TENNYSON -> 6.218092707399592e-11\n",
      "HEATBLAST -> 2.1700558136217296e-05\n",
      "HEX -> 0.002418251009657979\n",
      "HIGHBREED -> 0.00043029928929172456\n",
      "HUMUNGOSAUR (ANGRY) -> 0.004705017898231745\n",
      "HUMUNGOUSAUR -> 0.0005989822675473988\n",
      "JETRAY -> 0.004700044635683298\n",
      "KAI GREEN -> 8.278929143221093e-16\n",
      "KEVIN -> 0.03866412118077278\n",
      "KEVIN ELEVEN -> 0.00045117540867067873\n",
      "KICKIN HAWK -> 0.02527577243745327\n",
      "MICHAEL MORNINGSTAR -> 0.0018483659951016307\n",
      "MUTATED KEVIN -> 0.01139182411134243\n",
      "NRG -> 0.001166273606941104\n",
      "PARADOX (CANT INTERFERE) -> 0.003701953450217843\n",
      "RATH -> 0.00617988919839263\n",
      "RIPJAWS -> 8.390485163545236e-05\n",
      "SPIDERMONKEY -> 5.287782187224366e-05\n",
      "STINKFLY -> 1.55211843377856e-08\n",
      "SWAMPFIRE -> 0.0001230811030836776\n",
      "TERRASPIN -> 7.988775223566336e-07\n",
      "TOEPICK -> 1.0434857838914624e-10\n",
      "ULTIMATE AGGREGOR -> 0.0073524219915270805\n",
      "ULTIMATE ARTIGUANA -> 0.08878583461046219\n",
      "ULTIMATE BEN -> 0.11302780359983444\n",
      "ULTIMATE BIG CHILL -> 0.010684167966246605\n",
      "ULTIMATE CANNONBOLT -> 0.0025111325085163116\n",
      "ULTIMATE ECHOECHO -> 0.011875060386955738\n",
      "ULTIMATE GRAVATTACK -> 0.0004266769392415881\n",
      "ULTIMATE HUMONGOUSAUR -> 0.07918175309896469\n",
      "ULTIMATE KEVIN -> 0.009269298985600471\n",
      "ULTIMATE SPIDERMONKEY -> 0.010874919593334198\n",
      "ULTIMATE SWAMPFIRE -> 0.003179305698722601\n",
      "ULTIMATE WAYBIG -> 0.04520677030086517\n",
      "ULTIMATE WILDMUTT -> 0.00041360981413163245\n",
      "ULTIMOS -> 2.6294688382222375e-07\n",
      "UPCHUCK -> 1.018842649136431e-11\n",
      "UPGRADE -> 7.460554743432546e-12\n",
      "VILGAX -> 3.2841503821989804e-11\n",
      "VILGAX (10 HEROES) -> 7.350223768298747e-06\n",
      "VILGAX (9 HEROES) -> 4.488068938712786e-08\n",
      "VILGAX (ENHANCEMENTS) -> 2.799655618446195e-09\n",
      "VILGAX DAAGONS HEROLD -> 9.273675823351368e-05\n",
      "VILGAX W/ ASCALON -> 4.857181920669973e-06\n",
      "WATERHAZARD -> 1.9137608164854214e-12\n",
      "WAYBIG -> 0.00044090847950428724\n",
      "WHAMPIRE -> 2.2836974977735736e-08\n",
      "WILDMUTT -> 1.0079949358583903e-15\n",
      "WILDVINE -> 3.751586347494631e-17\n",
      "XLR8 -> 1.1859828440499709e-12\n",
      "YOUNG MAX -> 1.9265081134113092e-17\n"
     ]
    }
   ],
   "source": [
    "def test():\n",
    "    for i, j in enumerate(model.predict(x_test[0].reshape(1,-1))[0]):\n",
    "        print (\"{} -> {}\".format(mapping_character[i], j))\n",
    "\n",
    "        \n",
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 575,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(59, 26, 5.298317366548036, 2.995732273553991, 59)"
      ]
     },
     "execution_count": 575,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "return_individual_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 557,
   "metadata": {},
   "outputs": [],
   "source": [
    "vtok = {}\n",
    "ktov = get_power_level_mapping(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 558,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 577,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.995732273553991"
      ]
     },
     "execution_count": 577,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ktov[26]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 582,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
       "        17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
       "        34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50,\n",
       "        51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67,\n",
       "        68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84,\n",
       "        85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96]),)"
      ]
     },
     "execution_count": 582,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(df['Character'] == df['Character'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 762,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 769,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.2 64-bit",
   "language": "python",
   "name": "python38264bit0262eaa34bcb4dc5a729a50b53fd224d"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
